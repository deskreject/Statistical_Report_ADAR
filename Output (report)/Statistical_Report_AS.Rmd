---
title: "Statistical_Report_ADAR"
author: "Alexander Staub"
date: "September 11, 2020"
output: 
  html_document:
    code_folding: "hide"
    includes:
      after_body: footer.html
bibliography: C:\\R work\\Advanced data analytics with R\\Statistical Report\\references\\reference_list.bib        
---



Need to have an area of package loading and dataframe loading

# Abstract
[@Cameron.1994; @Craig.2017]

# Introduction

Online communities are developing into an area of increasing relevance for both practice and academic research in fields of economics, management and information systems management, to name a few. The reason being that the proliferation of the internet has reached nearly every corner of the world (Source - Cia factbook?) and is touching and forming an increasing number of areas relevant to work and life in general. Social Media platforms have developed from means to connect to old and new friends, to sources of news, organizing online and offline gatherings and conducting commerce. Special interest groups, from programming to brand enthusiasts, have moved from communicating and exchanging knowledge via listserves and e-mails, to creating and sharing everything from software code (Source on open source) to designs for physical objects (source for 3d printing papers) via dedicated virtual spaces hosted online. As it is reasonable to assume that the trend of interacting and communicating with eachother virtually and unimpeded by physical borders or geographical distance won't be declining in the forseeable future, it is essential to understand the behavior of individuals as well as groups in this relatively new enviornment.

First off, there are a number of different uses and definitions of the term online community, as well as different names for similar concepts, depending on the discipline or research question of interest. The research herein is concerned with the behavior of members creating and sharing knowledge within online communities seen as a new form of organization (@Puranam.2014).  They are broadly defined as virtual space of communication among members facilitated by ICT technology (Source), have a shared goal, mission or interests that can be centered around a particular product or an entire subject area (Source), which have porous or open organizational boundaries allowing for self selection in and out() and whose primary goal is to foster the collaborative and cummulative creation and sharing of information and knoweldge (Source). Hence, the context of relevance is distinct from social media platforms like Twitter and Facebook, that are means of connecting and sharing information with a (relatively) closed group of friends and acquaintances, as well as from temporary forms of organizing like crowd sourcing contests. 

Research in the area of online communities as defined above, has provided insight into, among other things, the design (Source), evolution (Source), outputs (Stanko) and governance (Lerner and Tirole source) of these new forms of organising. Regarding the latter aspect, one of the key questions that arises is how to steer the behavior of members in online communities, as online communities thrive and decline with the engagement of their members (Source - Oh and Jeon, Ma and Agrawal? Lappas?). This challenge is especially pressient for online communities, as contribution behavior in online commmunities is highly right skewed, i.e. a very small number of highly active and intrinsically motivated members is responsible for the majority of contributions in online communities (Source). Hence, the question of how to incentivize increased activity by a larger portion of participant is a natural consequence (Lurking sources). As opposed to closed forms of organising, where effort can be induced by monetary compensation and hierarchy, online communities' characteristics, like their potential unlimited size and fluid membership of participants (Source), make monetary incentives and hierarchical control less of a viable option. A steering mechanism that is commonly found in many online communities are symbolic awards in the form of virtual badges, medals, ranks, points etc (Source). 

Studies in this area of inquiry have investigated the impact of symbolic awards in online communities in the past. Therein it was found that the prospect of receiving rewards lead to postivie effects in terms of contribution behavior in the short (Park?) and long term (Source?), as well as mixed findings when investigating different types of members within communities (Goes?) or when investigating the distribution of activity types (Anderson). In addition, the reception of symbolic awards was found to lead to a decrease in activity levels when these awards were awarded conditional on fulfilling predetermined criteria (Goes?, Anderson?), while leading to sustained increase in knoweldge contributions when receiving the award was unexpected (Gallus). .. #More insights necessary.
What remains largely unkown however is how the introduction of a symbolic rewards impacts the contribution behavior of members within online community. This is of theoretical interest, as it would help understand how members of an organization react when previously non-incentivized activities are explicitely, symbolically rewarded. This would provide novel and relevant insights compared to previous studies, which have mainly focused on contexts where rewards for certain behaviors were already established and may have influenced the make-up of the community via self selecting in or out of the community by members based on the governance mechanisms. Furthermore, it would provide insight into a key organising challenge within new forms of organising as proposed by Puranam et al (2015), by ... (From a pervious abstract).

The goal of this statistical report is to investigate the effect that the introduction of conditional symbolic awards has on the knowledge contributions that users produce in response to questions posed by other users. Knowledge sharing in the form of answers is a common form of communication in online communities, some of the notable context examples which were the subject of scientific inquiry in the past include Sourceforge (Source), Quora, StackOverflow (Source) or Yahoo Answers (Source). Answering questions of fellow members requires at least some degree of time and effort commited by the answerer, without incurring any immedate or measurable benefit, while the question poser is rewarded with a potential solution for a problem. Reasons for users providing this effort have been connected to intrinsical motivation like the joy of problem solving or learning (source), as well as extrinsic benefits like reputational gain (Source) or deferred career benefits (Source). While it has been shown that the prospect of receiving a reward, even when it is intangible and non-monetary, can increase the effort provided by members (Source), there is reason to assume that the availbility of such rewards may reduce the contributions on average. Extrinsic incentives have been shown to lead to a decrease of intrinsic motivation among prospective as well as actual recipients in a number of settings (Source). This is especially salient when the goals to achieve set out by such conditional awards are lower than the orinal effort level by certain individuals, leading to a "moral licensing" or "done enough" effect (Source). Furthermore, it has been found that while such conditional awards can temporarily lead to a surge in rewarded activity, the levels tend to drop off after they receive the reward. Investigating this question within online communities will be of paritcular interest, as outlined above, contributions to these communities tend to be driven by a small group of committed members, who studies found to be largely intrinsically motivated (Von krogh source etc). Thus, this leads to the following hypothesis:
- The introduction of symbolic awards rewarding the fulfilment of predetermined criteria (conditional symbolic awards) will lead to a decline of the average answers provided by existing members of a community
 
 
In order to answer the research question of relevance, a natural experiment is exploited, in which a set of symbolic awards in the form of virtual badges was introduced into an online community focused on sharing knowledge and solving problems in the field of electrical engineering, was introduced on the 13th of April 2015. A difference in difference design is adopted, employing panel data on the user week-leve across 6 months before and after the introduction. The StackExchange Electrical Engineering online community was identified as a sufficently similar control community, which did not witness any change in their reward system during the time of relevance. Procedures like coding of individual members by individuals external to the research team, focusing on registered users and propensity score matching are employed to strengthen the identification strategy. 

# Data collection

```{r warning=FALSE, message=FALSE}
#loading the packages necessary for the entire analysis
library(tidyverse)
library(gridExtra)
library(lubridate)
library(Hmisc)
if (!require(psych)) install.packages("psych"); library(psych)  
if (!require(janitor)) install.packages("janitor"); library(janitor)  #for tabyl function
if (!require(RColorBrewer)) install.packages("RColorBrewer"); library(RColorBrewer)  
if (!require(MASS)) install.packages("MASS"); library(MASS)  #for nb.glm (from glm models.R)
if (!require(car)) install.packages("car"); library(car)  #for additional lm and glm plots and functions (from glm models.R)
if (!require(effects)) install.packages("effects"); library(effects)  #for plotting effects (from glm models.R)
if (!require(AER)) install.packages("AER"); library(AER)  #overdispersion test (from glm models.R)
if (!require(stargazer)) install.packages("stargazer"); library(stargazer)  #tables
if (!require(plm)) install.packages("plm"); library(plm)  #balanced panels
if (!require(anytime)) install.packages("anytime"); library(anytime) 
if (!require(tidyr)) install.packages("tidyr"); library(tidyr) #to get the "pivot_wider" 
if (!require(MatchIt)) install.packages("MatchIt"); library(MatchIt) #package for matching
if (!require(optmatch)) install.packages("optmatch"); library(optmatch) #package for optimal matching
if (!require(broom)) install.packages("broom"); library(broom) #for understanding regression results
if (!require(fixest)) install.packages("fixest"); library(fixest) #for the point estimate and error bar plots for pretrends
if (!require(lfe)) install.packages("lfe"); library(lfe) #for linear models with multiway clustering and fixed effects
if (!require(alpaca)) install.packages("alpaca"); library(alpaca) #for non-linear models with multiway clustering and fixed effects
if (!require(boot)) install.packages("boot"); library(boot) #for bootstrapped standard errors
if (!require(clusterSEs)) install.packages("clusterSEs"); library(clusterSEs) #for bootstrapped standard errors
if (!require(pscl)) install.packages("pscl"); library(pscl) #for the zero inflated and hurdle models
if (!require(vcd)) install.packages("vcd"); library(vcd) #for count regression diagnostics
if (!require(vcdExtra)) install.packages("vcdExtra"); library(vcdExtra) #for count regression diagnostics

#loading the necessary dataframes
load("C:/R work/Advanced data analytics with R/Statistical Report/data/w_panel_did_qual_as_regUs_6m_b.Rda")
load("C:/R work/Advanced data analytics with R/Statistical Report/data/qual_pre_panel_clean_coded.Rda")
load("C:/R work/Advanced data analytics with R/Statistical Report/data/qual_pre_panel_ee_coded.Rda")
load("C:/R work/Advanced data analytics with R/Statistical Report/data/crosssec_both_match.Rda")
```

To conduct this research, digital trace data was collected on the treatment and control community in two different ways. While Stackoverflow generously provides data dumps of every sub community, of which the electrical engineering community is one, data on the treatment community had to be scraped. A research assistant was tasked with writing the scraping program in order to get access, which was audited by the research team. For both communities, data was gathered on the post level, and then aggregated to the user-week level in order to provide the necessary analyses for this study. 

The control community was identified as relevant for a difference in difference study due to a number of factors. First of all, descriptions of the two communities were inspected by the researchers, which allowed the team to conclude that the topical focus of the two matched. While it has to be mentioned, that the main difference between the two communities is that the treatment community is owned and operated by a commercial parent company, the researchers observed that the parent company does not excert control over content and interactions in the community. This conclusion was coroborated by annecdotal evidence from the community manager. Secondly, tags used in posts made in each community were compared to assess whether there are large topical differences in the main topics that are discussed, which could be disconfirmed. Finally, the analysis is focused on the Q&A portion of the treatment community, which closely mirrors the mode of information exchange on StackOverflow.

To investigate the research question of how the introduction of symbolic awards impacts the contribution behavior of members, we set the time period of relevance to 6 months before and after the time of the introduction (the shock). We were able to isolate the introduction of the shock to the 13th of April 2015 due to a blog post published in the treated community. We also received annecdotal evidence from the community manager that the members of the commmunity were not aware of the impending change to the reward system, giving us confidence that we can rule out any potential anticipatory effect. At the same time the blog post itself was published by the community manager and received a considerable amount of views and comments, in addition to these new badges being added to a section of every members profile. 6 months around the time of introduction was chosen as a suitable time frame as it allowed enough time for users to get acquainted with the new rewards, while not too much time so as to risk other factors influencing our results. 

As mentioned briefly above, the sample being analyzed consists of users that were registered before the introduction of the symbolic awards and observations were aggregated to the user week level. We incldued only registered users as the focus of this study is on the behavioral changes of existing users of the community. We aggregated the panel data to a weekly level as this was the lowest possible frequency which allowed for measurable variation on the user level, given that a large portion of community members participate very infrequently. All in all, the sample includes 913 individual users in the treatment group and 2536 users in the control group across the period of relevance. After balancing the panel, each user has 48 observations, leading to a panel data set of 165.588 user-week observations, 9.995 of which are non-0 observations.  
```{r, message = F}
#number of users in the treated group
w_panel_did_qual_as_regUs_6m_b %>% filter(group==1) %>% group_by(user_ID) %>% dplyr::summarize(actions = n()) %>% glimpse()
#number of users in the control group
w_panel_did_qual_as_regUs_6m_b %>% filter(group==0) %>% group_by(user_ID) %>% dplyr::summarize(actions = n()) %>% glimpse()
```
Variables that are included in the panel data set that are relevant for the analysis within this statistical report include:

*   user_ID: a value indicating the identifier of each individual member
*   t_shock_w: a variable indicating each week before and after the week of treatment introduction, ranging from -24 to 23, 0 indicating the week of treatment introduction. 
* group: a dummy variable taking on the value 1 if an observation is part of the treated community
* post_treatment: a dummy variable taking on the value 1 if an observation was made after the week of the treatment introduction
* actions: the number of answers provided by a user in a week, which is the dependent variable of interest
* questions_per_week: a control variable indicating the amount of questions provided to the community in a given week, to control for any increase or decrease in answering behavior due to a increase or decrease in the supply of questions. 

There are no NA values present in the panel data as for the amount of answers variable, NAs were coerced into 0 after balancing.

A large portion of the data cleaning process was left out of this statistical report, as the data provided had to undergo a lot of preporsseing. A number of users were excluded from the panel data set based on coding by 3 research assistants, that filtered out users which were community moderators or employees and accounts that appeared to be purely of commercial purpose (linking to their own products or websites in every answer) - approximately 1.5% of users. Answers had to be coded as such, as in the treatment community individuals were able to comment on various kinds of content beyond questions, while in the control community, questions where the only form of original content contributions that illicted answers. Furthermore, a large amount of registered users in the treatment community had to be filtered out, as there seemed to have been a disproportionate amount of sudden registrations several years before the introduction of the symbolic awards that never actively contributed (approximately 150.000 within a week). The aggregation of the data to the user-week level was intentionally left out as well as the week variables and treatment variable had to be added to both community data sets. What will be shown in the following section is the propensity score matching procedure, conducted in order to aid in the identification of causal effects by making the treatment and control group more comparable. 

# Descriptive Analysis of the Sample

Within this section of the statistical report, an overview of the descriptive statistics will be provided for the dataset before and after the matching procedure. As the matching procedure is undertaken on the crosssectional level, including only observations before the shock, descritpive statistics of the matching variables are also provided. 2 different panel data sets will be used in the analysis portion of this report in addition to the unmatched panel data set

### Descriptive statistics unmatched

Below are tables of the descriptive statiscs before and after the introduction of the treatment for each group. It can be seen in the tables that there is a large reduction in the number of answers per uesr as well as questions posed per week in the treated community after the introduction of the shock. In the control community a reduction in the answers can be seen as well, however there is only a marginal change in the amount of questions posed per week. Furthermore, there is a large difference in the median value of questions asked between the two groups, which is not the case when insepcting the median answers per users. 

```{r, results = 'asis'}
#descriptives for the treatment group, before treatment
  stargazer(w_panel_did_qual_as_regUs_6m_b %>% filter(group == 1, post_treatment == 0) %>% dplyr::select(t_shock_w, post_treatment, actions, questions_per_week),
            type = "html",
            title ="Descriptive Statistics treated community, before shock",
            digits = 2,
            font.size = "normalsize",
            single.row = TRUE,
            notes = "",
            column.separate = c(4, 1, 1, 1, 1, 1),
            summary.stat = c("n","mean","sd", "min","max", "median"))

#descriptives for the treatment group, after treatment
  stargazer(w_panel_did_qual_as_regUs_6m_b %>% filter(group == 1, post_treatment == 1) %>% dplyr::select(t_shock_w, post_treatment, actions, questions_per_week),
            type = "html",
            title ="Descriptive Statistics treated community, after shock",
            digits = 2,
            font.size = "normalsize",
            single.row = TRUE,
            notes = "",
            column.separate = c(4, 1, 1, 1, 1, 1),
            summary.stat = c("n","mean","sd", "min","max", "median"))

#descriptives for the control group, before treatment
  stargazer(w_panel_did_qual_as_regUs_6m_b %>% filter(group == 0, post_treatment == 0) %>% dplyr::select(t_shock_w, post_treatment, actions, questions_per_week),
            type = "html",
            title ="Descriptive Statistics control community, before shock",
            digits = 2,
            font.size = "normalsize",
            single.row = TRUE,
            notes = "",
            column.separate = c(4, 1, 1, 1, 1, 1),
            summary.stat = c("n","mean","sd", "min","max", "median"))

#descriptives for the control group, after treatment
  stargazer(w_panel_did_qual_as_regUs_6m_b %>% filter(group == 0, post_treatment == 1) %>% dplyr::select(t_shock_w, post_treatment, actions, questions_per_week),
            type = "html",
            title ="Descriptive Statistics control community, before shock",
            digits = 2,
            font.size = "normalsize",
            single.row = TRUE,
            notes = "",
            column.separate = c(4, 1, 1, 1, 1, 1),
            summary.stat = c("n","mean","sd", "min","max", "median"))
```

### Graphical Representation of the Dependent Variable

Investigating the dependent variable graphically shows that there is a very large representation of 0s in the dataset, which makes it hard to compare the treated and control community visually

```{r}
#densuty plot of the two groups answering behavior, before and after the treatment
w_panel_did_qual_as_regUs_6m_b %>%
  ggplot(aes(x = actions)) +
  geom_density() + 
  ggtitle("distribution of answers by user-week level, before and after treatment") + 
  facet_grid(group~post_treatment, labeller = label_both)
```

a log transformation does not yield any clear improvement in terms of making the distribution of the DV more normal. 
```{r}
#density plot of the two groups, before and after treatment, logged DV
w_panel_did_qual_as_regUs_6m_b  %>%
  ggplot(aes(x = log(actions+1))) +
  geom_density() + 
  ggtitle("distribution of logged answers by user-week level, before and after treatment") + 
  facet_grid(group~post_treatment, labeller = label_both)
```

when excluding 0s from the dataset, the distribution is still very right skewed.   

```{r}
#densuty plot of the two groups answering behavior, before and after the treatment
w_panel_did_qual_as_regUs_6m_b  %>% filter(actions > 0) %>%
  ggplot(aes(x = actions)) +
  geom_density() + 
  ggtitle("distribution of answers, excluding zeros, by user-week level, before and after treatment") + 
  facet_grid(group~post_treatment, labeller = label_both)
```

The inspection of the descriptive statistics and visual representation of the DV suggests that a count regression specification will be necessary to make valid inferences, whereas a poisson regression might not be the best option as the mean (0) does not equal the variance. Before running the analysis, a propensity score matching procedure is implemented in order to make the two groups more homogeneous. 

### propensity score matching

In the following section, the propensity matching procedure is undertaken and described, using matching variables based on the available data. Unfortunately, there is no demopgraphic data available on the user level, which is why information on the contribution behavior is used to undertake the propensity score matching. 

The preparation of the data used to conduct the propensity was done outside of the statistical report. The reason being is that while the analysis is being done on panel data, the matching was conducted with crosssectional data. The data for the crosssectional matching procedure is based on the following variables: 

*   Days since a user was registered to the community
*   average words per answer
*   number of original posts made by a user between registration and time of incentive introduction
*   overall number of answers made by a user between registration and time of incentive introduction
*   number of answers made by a user 1 year before the incentive introduction
*   number of answers made by a user 2 years before the incentive introduction
*   number of answers made by a user 3 years before the incentive introduction

These variables are a subset of variables chosen to base the propensity score matching on. The original set of variables were chosen based on whether they may influence selection into treatment as well as based on the data that was available for users of both communities. The variables that were dropped in favor of the final subset included the number of questions made between registration and time of introduction, number of views that an original post received and number of words of original posts, as well as 3 year lags of all of the above. These variables were dropped as they did not provide any improvement in the model fit when regressing the treatment variable on combinations of variables which included the aforementioned. The analysis of the different models is left out of this report for conciseness reasons. Given the right skewedness in the distribution of many of the variables, matching models were calculated with the above variables in unaltered form, as well as after creating quantiles for each of the variables. The reason for matching on quantiles is due to the fact that nearest neighbor matching may not be very precise when observations of the treated group that lie at the extremes of the variable distributions lead to a very high propensity score, which may not be achieved by any observation in the control treatment (this effect is observed to a small degree and illustrated in the matching procedures that follow)  The quantiles vary in level given the fact that there are large differences in the distributions of the variable. For example, only 2 levels were able to be created from the "answers 3 years before the treatment introduction" variable, as the vast majority of users were not active 3 years before the shock. Similarly, the "original posts" variable only has 2 levels for the same reason, that most members did not produce any original content, while a small number preferred original contributions over providing answers to other user's contributions. A version with standardized variables was also calculated, which did not provide any significant improvement in the fit of the model. Below, the fit of the 2 final models, chosen based on the lowest AIC, regressing the treatment dummy on both the quantiled as well as unaltered variables is presented

```{r, warning=FALSE, message=FALSE}
#quantile propensity score calculations
m_ps_qs_2 <- glm(t_groups ~ d_since_reg_qs_2 + 
                   orig_posts_qs_2 +
                   words_p_answer_qs_2 +
                   answers_total_qs_2 + 
                   answers_t_1_qs_2 +
                   answers_t_2_qs_2 +
                   answers_t_3_qs_2,
                 family = binomial(), data = crosssec_both_match)

#unaltered propensity score calculations
m_ps_unalt_1 <- glm(t_groups ~  d_since_reg +
                      orig_posts +
                      words_p_answer +
                      answers_total +
                      answers_t_1 + 
                      answers_t_2 + 
                      answers_t_3, family = binomial(), data = crosssec_both_match)

summary(m_ps_qs_2)
summary(m_ps_unalt_1)
```

In the quantiled version of the variables, in which each quantile is treated as a factor, total number of answers variable was highly significant in predicting whether a user resembled belonging to the treated community. The negative coefficient shows that user in the untreated community tend to contribute more answers than users in the treated community. Words per answer on the other hand was a weak predictor for wether a user was resembled being part of the treated community, implying that the number of words per answer is relatively similar for users in either community. Looking at the coefficients of the model including the variables in continuous (unaltered) form, every variable is significant at least at the 90% confidence level. 

For the matching procedure, a number of different options are available, including nearest neighbor optimal, nearest neighbor ordered and coarsened exact matching just to name a few. Similar to the case above, the selection process was based on running various matching procedures with different combinations of variables, in unaltered, qunatiled and standardized form. Once again, only the final matching procedures are presented in this report, which are ordered nearest neighbor matching with the combination of variables as presented above, in unaltered and quantiled form. Here, ordered refers to the fact that the observations were sorted in descending order according to their propensity scores, which were predicted using the regression models from above. 

What follows is the ordered nearest neighbor matching procedure using the variables in their unaltered, continuous form

```{r, warning=FALSE, message=FALSE}
##execute NN matching ordered for un-quantiled, un-scaled with best fitting model
crosssec_both_match$PS_unalt <- predict(m_ps_unalt_1)
crosssec_both_match_unalt_ord <- crosssec_both_match %>% arrange(desc(PS_unalt))

#execute the matching by nearest neighbour
crosssec_both_match_unalt_ord_NoNas <- crosssec_both_match_unalt_ord %>% dplyr::select(c(-views_p_post, -p_views_qs, -p_views_qs_2, -words_p_all_posts))

NN_match_unalt_ord <- matchit(t_groups ~  d_since_reg +
                                orig_posts +
                                words_p_answer +
                                answers_total +
                                answers_t_1 + 
                                answers_t_2 + 
                                answers_t_3, method = "nearest", data = crosssec_both_match_unalt_ord_NoNas)

summary(NN_match_unalt_ord)
```
from the above matching output, when comparing the mean differences and balance improvements of the control group, it can be seen that a considerable improvement in the balance of the matching variables has been achieved with the matching procedure. 
Below, a visual representation of the propensity score changes before and after matching for the matching procedure based on the unaltered, continuous variables, is provided. 

```{r}
plot(NN_match_unalt_ord, type = "jitter", interactive = F)
plot(NN_match_unalt_ord, type = "hist")
```


The visual comparison shows a far more balanced distribution of the propensity scores after matching. The first graphic clearly shows that a large number of observations with very low propensity scores was purged from the control group. However, there remain a number of observations with very high propensity scores in the treatment group, which were not able to be matched with observations in the control group that had a similarly high propensity score. 

Next, the ordered nearest neighbor matching procedure using the variables as coerced into factors according to quantiles is presented

```{r}
crosssec_both_match$PS <- predict(m_ps_qs_2)

crosssec_both_match_ord <- crosssec_both_match %>% arrange(desc(PS))

##execute the matching by nearest neighbor with quantiles after ordering
matching_panel_both_ord_noNas <- crosssec_both_match_ord %>% dplyr::select(c(-views_p_post, -p_views_qs, -p_views_qs_2, -words_p_all_posts))

NN_match_qs_ord <- matchit(t_groups ~ d_since_reg_qs_2 + 
                             orig_posts_qs_2 +
                             words_p_answer_qs_2 +
                             answers_total_qs_2 + 
                             answers_t_1_qs_2 +
                             answers_t_2_qs_2 +
                             answers_t_3_qs_2, method = "nearest", data = matching_panel_both_ord_noNas)

summary(NN_match_qs_ord)

```
The balance improvement using the factor variables is not as clear when taking the "percent balance improvement" table at face value. However it has to be said that the number of variables to match on increased significantly due to every factor level being matched on. In addition, the absolute differences are far smaller before and after the matching procedure when inspecting means in the first two tables. Overall, the balance improved for 11 of 19 factor levels. 

```{r}
plot(NN_match_qs_ord, type = "jitter", interactive = F)
plot(NN_match_qs_ord, type = "hist")
```


Visually on the other hand, the balance improvement after matching is promising when inspecting the propensity scores. As before, a large number of control group observations with low propensity scores were purged from the dataset, while at the same time the high propensity score observations of the treatment group could be matched more closely with control group observations compared to the matching based on the unaltered, continuous variables. Comparing the histograms in the second graphic also illustrates the considerable improvements in balance of the two groups due to the matching procedure

### descriptives - panel data including only matched observations

Following the comparison of the propensity scores before and after matching for the control and treated group, it makes sense to inspect whether this improvement in balance translates into improved balance in the descriptive statistics, at least before the introduction of the treatment. Below an overview of the descriptive statistics for the panel data based on the matching procedure with the unaltered, continuous variables is provided. Given the visual inspection was not particularly helpful due to the large right skew of the dependent variable, only the descriptive statistics tables are provided

```{r, results = 'asis'}
#extracting the matching data
match_data_unalt <- match.data(NN_match_unalt_ord)

#limiting the panel data to the matched unaltered users
w_panel_matched_qual_6m_unalt_b <- w_panel_did_qual_as_regUs_6m_b %>% filter(user_ID %in% match_data_unalt$user_ID)

#providing the descriptive statistics
  stargazer(w_panel_matched_qual_6m_unalt_b %>% filter(group == 1, post_treatment == 0) %>% dplyr::select(t_shock_w, post_treatment, actions, questions_per_week),
            type = "html",
            title ="Descriptive Statistics treated community, before shock",
            digits = 2,
            font.size = "normalsize",
            single.row = TRUE,
            notes = "",
            column.separate = c(4, 1, 1, 1, 1, 1),
            summary.stat = c("n","mean","sd", "min","max", "median"))

#descriptives for the treatment group, after treatment
  stargazer(w_panel_matched_qual_6m_unalt_b %>% filter(group == 1, post_treatment == 1) %>% dplyr::select(t_shock_w, post_treatment, actions, questions_per_week),
            type = "html",
            title ="Descriptive Statistics treated community, after shock",
            digits = 2,
            font.size = "normalsize",
            single.row = TRUE,
            notes = "",
            column.separate = c(4, 1, 1, 1, 1, 1),
            summary.stat = c("n","mean","sd", "min","max", "median"))

#descriptives for the control group, before treatment
  stargazer(w_panel_matched_qual_6m_unalt_b %>% filter(group == 0, post_treatment == 0) %>% dplyr::select(t_shock_w, post_treatment, actions, questions_per_week),
            type = "html",
            title ="Descriptive Statistics control community, before shock",
            digits = 2,
            font.size = "normalsize",
            single.row = TRUE,
            notes = "",
            column.separate = c(4, 1, 1, 1, 1, 1),
            summary.stat = c("n","mean","sd", "min","max", "median"))

#descriptives for the control group, after treatment
  stargazer(w_panel_matched_qual_6m_unalt_b %>% filter(group == 0, post_treatment == 1) %>% dplyr::select(t_shock_w, post_treatment, actions, questions_per_week),
            type = "html",
            title ="Descriptive Statistics control community, before shock",
            digits = 2,
            font.size = "normalsize",
            single.row = TRUE,
            notes = "",
            column.separate = c(4, 1, 1, 1, 1, 1),
            summary.stat = c("n","mean","sd", "min","max", "median"))
  
```


As shown in the descriptive statistics table, there is little improvement in terms of similarity of means in the DV as well as the control variable, questions per week. However, the standard deviations for the DV in the treated and control community are more similar than before.

Finally, the descriptive statistics for the treated and control community are provided based on observations that were matched on the factor variables constructed from quantiles of the matching variables. 

```{r, results= 'asis'}

#extracting the matching data
match_data_qs <- match.data(NN_match_qs_ord)

#including only the observations of the matching procedure
w_panel_matched_qual_6m_qs_b <- w_panel_did_qual_as_regUs_6m_b %>% filter(user_ID %in% match_data_qs$user_ID)

#providing the descriptive statistics
  stargazer(w_panel_matched_qual_6m_qs_b %>% filter(group == 1, post_treatment == 0) %>% dplyr::select(t_shock_w, post_treatment, actions, questions_per_week),
            type = "html",
            title ="Descriptive Statistics treated community, before shock",
            digits = 2,
            font.size = "normalsize",
            single.row = TRUE,
            notes = "",
            column.separate = c(4, 1, 1, 1, 1, 1),
            summary.stat = c("n","mean","sd", "min","max", "median"))

#descriptives for the treatment group, after treatment
  stargazer(w_panel_matched_qual_6m_qs_b %>% filter(group == 1, post_treatment == 1) %>% dplyr::select(t_shock_w, post_treatment, actions, questions_per_week),
            type = "html",
            title ="Descriptive Statistics treated community, after shock",
            digits = 2,
            font.size = "normalsize",
            single.row = TRUE,
            notes = "",
            column.separate = c(4, 1, 1, 1, 1, 1),
            summary.stat = c("n","mean","sd", "min","max", "median"))

#descriptives for the control group, before treatment
  stargazer(w_panel_matched_qual_6m_qs_b %>% filter(group == 0, post_treatment == 0) %>% dplyr::select(t_shock_w, post_treatment, actions, questions_per_week),
            type = "html",
            title ="Descriptive Statistics control community, before shock",
            digits = 2,
            font.size = "normalsize",
            single.row = TRUE,
            notes = "",
            column.separate = c(4, 1, 1, 1, 1, 1),
            summary.stat = c("n","mean","sd", "min","max", "median"))

#descriptives for the control group, after treatment
  stargazer(w_panel_matched_qual_6m_qs_b %>% filter(group == 0, post_treatment == 1) %>% dplyr::select(t_shock_w, post_treatment, actions, questions_per_week),
            type = "html",
            title ="Descriptive Statistics control community, before shock",
            digits = 2,
            font.size = "normalsize",
            single.row = TRUE,
            notes = "",
            column.separate = c(4, 1, 1, 1, 1, 1),
            summary.stat = c("n","mean","sd", "min","max", "median"))
```

As can be seen above, the mean as well as standard deviation of the dependent variable increased compared to before the matching procedure in the control group, both before and after incentive introduction. This may mean that the matching procedure made the two groups less comparable, or that there are substantial differences in levels of answering activity within the control group. This suggests that indivdiual fixed effects may need to be included in the difference in difference analysis using the panel data including observations from the matching procedure using quantiles. 

What follows in the next section will be the analysis of how the introduction of symbolic awards affects the answering behavior of members within online communities. The matching procedures illustrated above, in addition to the pre-processing of the raw data, provide further arguments for the identification of a generalizable causal effect. 



# Analysis

### hypotheses that will be tested


### Performance analysis

Given the non-negative integer values the dependent variable takes on, and taking into account the right tail skeweness of its distribution shown above, it makes sense to choose a model which is more suitable than an OLS regression specification. There are a number of options available that rely on more suitable distributional properties to model the residuals, which is why it makes sense to investigate the dependent variable in more detail before deciding on a specification. 

```{r}
dv <- w_panel_matched_qual_6m_unalt_b$actions

#show ord plot
Ord_plot(dv)

#the "xxxxness" plot, using poisson
distplot(dv, type = "poisson")

#the "xxxxxness" plot, using nbin
distplot(dv, type = "nbinom")

#the goodfit function poisson
summary(goodfit(dv))

#the goodfit function nbin
plot(goodfit(dv, type = "nbinom"))

#the goodfit funciton for binomial distribution
plot(goodfit(dv, type = "binom"))

```

- just the general model first. If I have time, I will try to write out the regression equation for the poisson models etc. 

```{r}

#### unaltered data ####
#run the logged and unlogged dv models

formula_reg <- actions ~ group*post_treatment + questions_per_week
formula_log_reg <- log(actions + 1) ~  group*post_treatment + questions_per_week
formula_did_coef <- actions ~ did_coeff + questions_per_week
formula_log_did_coef <- log(actions +1) ~ did_coeff + questions_per_week

#### matched on unaltered data
# data prep
w_panel_matched_qual_6m_unalt_b$did_coeff <- w_panel_matched_qual_6m_unalt_b$group*w_panel_matched_qual_6m_unalt_b$post_treatment 

### OLS regressions
#unlogged, no FEs
ols_matched_unalt_asw_6m <- lm(formula_reg, data = w_panel_matched_qual_6m_unalt_b)
#logged, no FEs
ols_logged_matched_unalt_asw_6m <- lm(formula_log_reg, data = w_panel_matched_qual_6m_unalt_b)
#logged, FEs and clustered standard errors
ols_logged_fullFEs_matched_unalt_asw_6m <- felm(log(actions + 1) ~  group*post_treatment + questions_per_week|user_ID + as.factor(t_shock_w)|0|user_ID, data = w_panel_matched_qual_6m_unalt_b)

### Count regressions
# poisson - without fixed effects
pois_matched_unalt_asw_6m <- glm(actions ~ group*post_treatment + questions_per_week, family = poisson, data = w_panel_matched_qual_6m_unalt_b)
# negative binomial - without fixed effects
(theta_matched <- glm.nb(actions ~ group*post_treatment + questions_per_week, data = w_panel_matched_qual_6m_unalt_b)$theta) 
negbi_matched_unalt_asw_6m <- update(pois_matched_unalt_asw_6m, family = negative.binomial(theta_matched))
# poisson hurdle - without fixed effects
pois_h_matched_unalt_asw_6m <- hurdle(actions ~ group*post_treatment + questions_per_week, data = w_panel_matched_qual_6m_unalt_b)
#negative binomial hurdle - without fixed effects
negbi_h_matched_unalt_asw_6m <- hurdle(actions ~ group*post_treatment + questions_per_week, data = w_panel_matched_qual_6m_unalt_b, dist = "negbin")

#poisson - with fixed effects
pois_matched_unalt_FEs_asw_6m <- fepois(actions ~ group*post_treatment + questions_per_week|as.factor(user_ID) + as.factor(t_shock_w), w_panel_matched_qual_6m_unalt_b)
#negative binomial - with fixed effects
negbi_matched_unalt_FEs_asw_6m_1 <- fenegbin(actions ~ did_coeff + questions_per_week|as.factor(user_ID) + as.factor(t_shock_w), w_panel_matched_qual_6m_unalt_b)
# poisson hurdle - with dummies for time fixed effects
pois_h_matched_unalt_FEs_asw_6m <- pscl::hurdle(actions ~ did_coeff + questions_per_week + as.factor(t_shock_w), data = w_panel_matched_qual_6m_unalt_b)
# negbi hurdle - with dummies for time fixed effects
negbi_h_matched_unalt_FEs_asw_6m <- pscl::hurdle(actions ~ did_coeff + questions_per_week + as.factor(t_shock_w), data = w_panel_matched_qual_6m_unalt_b, dist = "negbin")

etable(pois_matched_unalt_FEs_asw_6m,
          negbi_matched_unalt_FEs_asw_6m_1)

stargazer(ols_matched_unalt_asw_6m,
          ols_logged_matched_unalt_asw_6m,
          ols_logged_fullFEs_matched_unalt_asw_6m,
          pois_matched_unalt_asw_6m,
          negbi_matched_unalt_asw_6m,
          pois_h_matched_unalt_asw_6m,
          negbi_h_matched_unalt_asw_6m,
          type = "text")
```

The models above are presented to compare models with and without fixed effects, as well as models that are distributionally more suitable for the dependent variable of interest (count models, specifically the hurdle models) and ones that are less suitable. A hurdle model is chosen as appropriate vs a zero inflated model as all the observations are "at risk" of producing an answer, hence 0s are assumed to have arisen due to sampling only and not due to structural reasons (Rose et al 2006). 

When inspecting the coefficients, all models 
- the introduction of the incentives leads to a large increase in the 0s, which could imply that members left the community
- little change in the answering behavior of members that have an activity level - however does not include any fixed effects yet 
- this is without any robust clustered standard errors

Below a model comparison is provided, showing the different fit parameters used to assess which of the models to choose based on the fit parameters (AIC, BIC and LR ratio)

```{r}

#showing the fit of the count models
LRstats(ols_matched_unalt_asw_6m,
          ols_logged_matched_unalt_asw_6m,
          ols_logged_fullFEs_matched_unalt_asw_6m,
          pois_matched_unalt_asw_6m,
          negbi_matched_unalt_asw_6m,
          pois_h_matched_unalt_asw_6m,
          negbi_h_matched_unalt_asw_6m)

#showing the model summaries of the models that did not get recognized by LRstats
summary(pois_matched_unalt_FEs_asw_6m)
summary(negbi_matched_unalt_FEs_asw_6m_1)

```


From the model comparison above, looking at AIC & BIC values, a few observations can be made. On the one hand, the OLS model with the logged DV and FE included has the lowest AIC. 
- point out fact of lower DFs though due to individual fixed effects, which leads to exclusion of individuals who are not active in both periods
- point out fact that hurdle model makes sense from a distribution stand point, however assumes members are split into portion of people who would never answer, and people who would answer. Hence, the results have to be interpreted as looking at a subset of members that have a naturally higher rate of answering

#### Preconditions
need to run an qq plot to determine normality

```{r}
if (!require(countreg)) install.packages("countreg"); library(countreg)#Dangerous, as it overwrites the pscl hurdle models!
#diagnostics
## getting fitted values and comparing them to observed values
resids_ols_logged_FEs_m_unalt <- log(w_panel_matched_qual_6m_unalt_b$actions+1) - residuals(ols_logged_fullFEs_matched_unalt_asw_6m)

exp(max(resids_ols_logged_FEs_m_unalt))
#plot them
plot(resids_ols_logged_FEs_m_unalt, log(w_panel_matched_qual_6m_unalt_b$actions+1), asp = 1)
abline(0, 1, col = 'red', lty = 'dashed', lwd = 2)

#plot the residuals of the model with fixed effects
qqnorm(residuals(ols_logged_fullFEs_matched_unalt_asw_6m), ylab = 'Residuals')
qqline(residuals(ols_logged_fullFEs_matched_unalt_asw_6m))

#plot the residuals of the model without fixed effects
qqnorm(residuals(ols_logged_matched_unalt_asw_6m), ylab = 'Residuals')
qqline(residuals(ols_logged_matched_unalt_asw_6m))


#plot rootograms for different models, starting with poissons and NB models without fixed effects
countreg::rootogram(pois_matched_unalt_asw_6m)
countreg::rootogram(negbi_matched_unalt_asw_6m)

#rootograms for the hurdle models
countreg::rootogram(pois_h_matched_unalt_asw_6m)
countreg::rootogram(negbi_h_matched_unalt_asw_6m)

```



### Pretrend analysis

For the difference in difference coefficient to be interpreted as causal, the requirements are that the pretrends of the dependent variable are comparable in both groups. I.e. they don't need to be the same in level, however there should not be any serious deviations in changes over time between the two groups. Below, the pretrends are presented for the negative binomial regression specification. It does not match the specifications of the main model used, as the author was not able to make the pretrend analysis work with individual and time fixed effects. 

```{r}
#set up the model for the pretrend check - negative binomial, unaltered matched
pretrend_pois_m_unalt_asw_6m <- glm(actions ~ group*as.factor(t_shock_w) + questions_per_week, family = poisson, data = w_panel_matched_qual_6m_unalt_b) 
(theta_pretrend <- glm.nb(actions ~ group*as.factor(t_shock_w) + questions_per_week, data = w_panel_matched_qual_6m_unalt_b)$theta) 
pretrend_nb_m_unalt_asw_6m <- update(pretrend_pois_m_unalt_asw_6m, family = negative.binomial(theta_pretrend))

#negative binomial with fixed effects
pretrend_pois_FEs_m_unalt_asw_6m <- fepois(actions ~ group*as.factor(t_shock_w) + questions_per_week|as.factor(user_ID) + as.factor(t_shock_w), data = w_panel_matched_qual_6m_unalt_b)


#plot the pretrends
fixest::coefplot(pretrend_pois_FEs_m_unalt_asw_6m, keep = ":",
                                                            col = 4, 
                                                            main = "6 months trend of answers per user",
                                                            xlab = "interaction group x week",
                                                            ylab = "weekly answers / user")

```

For optimal pretrends one would want the confidence intervals in each period to cross the horizontal line at y=0, implying that statistically, difference in the change from period to period in each group is equal to 0 (see Barbosu and Gans, 2017). As is evident from the pretrend analysis above using the poisson regression with fixed effects on the week and individual level specification (as well as with the Negative Binomial specification), pretrends are not optimal when comparing the two groups, raising considerable doubt about whether the difference-in-difference coefficient can be interpreted as representing a causal effect of the symbolic award introduction. 

```{r}
#pretrends for answers using the log transformed dependent variable - including individual fixed effects

pretrends_asw_matched_unalt_log <- felm(log(actions+1) ~ group*as.factor(t_shock_w) + questions_per_week|as.factor(t_shock_w) + user_ID|0|user_ID, data = w_panel_matched_qual_6m_unalt_b)

summary(pretrends_asw_matched_unalt_log)

#plot the pretrends

tidy_log_pretrends <- tidy(pretrends_asw_matched_unalt_log, conf.int = T)

tidy_log_pretrends$week <-  c(-72:23)


tidy_log_pretrends %>% filter(is.na(estimate) == F, term != "questions_per_week") %>%
  ggplot(aes(x = week, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0, color = "red") + 
  labs(title = "Pretrend analysis logged answers \n (including time and individaul FEs + individual clustered standard errors) \n 6 months around shock") + 
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text = element_text(angle = 90))
  
```

The pretrends above, which are based on the model specification using the logged dependent variable, including individual and time fixed effects as well as clustering the standard errors on the individual, while not perfect, provide better arguments for interpreting the difference in difference coefficient of the model as causal (it should be noted here, that for the week*group interaction in period -18, no coefficient was calculated). 

In summary, while the distribution of the dependent variable suggests that a count model, specifically one that accounts for overdispersion and zero inflation, should be used, the pretrends do not allow for the causal interpretation of the DiD coefficient. Taking into account that the pretrends look more promising when using the logged DV specification for the pretrend analysis, in addition to the fact that the DiD coefficient is negative and significant in the model specifications without modelling the non-zeros seperately, the conclusion is drawn that the introduction of the symbolic awards leads to a reduction in the average answers per user. Pretrend checks for the hurdle model were not able to be shown as no coefficient could be calculated for the week*group interaction effect. Hence, until a solution is found for this issue, it can't be assumed that the positive change in contribution behavior suggested by the coefficients of the hurdle model are causally related to the introduction of the symbolic awards. 


# Concluding remarks

Criticisms:
Possibly an option to implement a 0 inflated poisson model, however the researchers were not able to determine how to implement the zeron inflated poisson models. In particular, how to model the probability of having answered or not

# References

